{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68a7514",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba729a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "import two_layer_net\n",
    "import common\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d84b2d",
   "metadata": {},
   "source": [
    "#### Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c90311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "    return 1/(1 + np.exp(-a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574544ae",
   "metadata": {},
   "source": [
    "#### Softmax Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1a8054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    a = np.exp(a - c)\n",
    "    s = np.sum(a)\n",
    "\n",
    "    return a/s \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cef74e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0] \n",
    "    return -np.sum(t * np.log(y + 1e-7))/batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "364db381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "\n",
    "    grad = np.zeros_like(x) \n",
    "\n",
    "    for idx in range(x.size):\n",
    "        # save x[idx]\n",
    "        tmp = x[idx]\n",
    "\n",
    "        # for f(x + h)\n",
    "        x[idx] = tmp + h\n",
    "        fh1 = f(x)\n",
    "\n",
    "        # for f(x - h)\n",
    "        x[idx] = tmp - h\n",
    "        fh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fh1 - fh2)/(2*h)\n",
    "        # restore x[idx]\n",
    "        x[idx] = tmp\n",
    "\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9b1520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    if x.ndim == 1:\n",
    "        return _numerical_gradient(f, x)\n",
    "    else:\n",
    "        grad = np.zeros_like(x)\n",
    "        for idx, x in enumerate(x):\n",
    "            grad[idx] = _numerical_gradient(f, x)\n",
    "\n",
    "        return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f845f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "        w1, w2 = self.params['w1'], self.params['w2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1 = np.dot(x, w1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "\n",
    "        a2 = np.dot(z1, w2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c4ff988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(self, x, t):\n",
    "    y = self.predict(x)\n",
    "    return cross_entropy_error(y, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578858a7",
   "metadata": {},
   "source": [
    "#### Defining Mnist Clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0379e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MnistData():   \n",
    "    image_size = 784  # 28x28\n",
    "    image_dim = (1, 28, 28)\n",
    "    train_num = 60000\n",
    "    test_num  = 10000\n",
    "    key_file = {\n",
    "        'train_images': 'C:/Users/13132/Documents/Pattern_Recoginition_NN/Homework_2/ece5831-2023-assignment-2/ece5831-2023-assignment-2/Assignment_6/Train && Test_Files/train-images-idx3-ubyte.gz',\n",
    "        'train_labels': 'C:/Users/13132/Documents/Pattern_Recoginition_NN/Homework_2/ece5831-2023-assignment-2/ece5831-2023-assignment-2/Assignment_6/Train && Test_Files/train-labels-idx1-ubyte.gz',\n",
    "        'test_images':  'C:/Users/13132/Documents/Pattern_Recoginition_NN/Homework_2/ece5831-2023-assignment-2/ece5831-2023-assignment-2/Assignment_6/Train && Test_Files/t10k-images-idx3-ubyte.gz',\n",
    "        'test_labels':  'C:/Users/13132/Documents/Pattern_Recoginition_NN/Homework_2/ece5831-2023-assignment-2/ece5831-2023-assignment-2/Assignment_6/Train && Test_Files/t10k-labels-idx1-ubyte.gz'\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def _load_images(self, file_name):\n",
    "        with gzip.open(file_name, 'rb') as f:\n",
    "            images = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        images = images.reshape(-1, self.image_size)\n",
    "\n",
    "        print('Done with loading images: ', file_name)    \n",
    "        return images\n",
    "\n",
    "\n",
    "    def _load_labels(self, file_name):\n",
    "        with gzip.open(file_name, 'rb') as f: \n",
    "            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        \n",
    "        print('Done with loading labels: ', file_name)    \n",
    "        return labels\n",
    " \n",
    "    \n",
    "    def _change_one_hot_label(self, x):\n",
    "        t = np.zeros((x.size, 10))\n",
    "        for idx, row in enumerate(t):\n",
    "            row[x[idx]] = 1\n",
    "\n",
    "        return t\n",
    "    \n",
    "\n",
    "    def load(self, normalize=True, flatten=True, one_hot_label=True):\n",
    "        dataset = {}\n",
    "        dataset['train_images'] = self._load_images(self.key_file['train_images'])\n",
    "        dataset['train_labels'] = self._load_labels(self.key_file['train_labels'])\n",
    "        dataset['test_images']  = self._load_images(self.key_file['test_images'])\n",
    "        dataset['test_labels']  = self._load_labels(self.key_file['test_labels'])\n",
    "        \n",
    "        if normalize:\n",
    "            for key in ('train_images', 'test_images'):\n",
    "                dataset[key] = dataset[key].astype(np.float32)\n",
    "                dataset[key] /= 255.0\n",
    "    \n",
    "        if one_hot_label:\n",
    "            dataset['train_labels'] = self._change_one_hot_label(dataset['train_labels'])\n",
    "            dataset['test_labels'] = self._change_one_hot_label(dataset['test_labels'])\n",
    "    \n",
    "        if not flatten:\n",
    "             for key in ('train_images', 'test_images'):\n",
    "                dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
    "    \n",
    "        return (dataset['train_images'], dataset['train_labels']), \\\n",
    "                (dataset['test_images'], dataset['test_labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce41c513",
   "metadata": {},
   "source": [
    "#### Two Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78873e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['w1'] = weight_init_std*np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['w2'] = weight_init_std*np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        w1, w2 = self.params['w1'], self.params['w2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1 = np.dot(x, w1) + b1\n",
    "        z1 = common.sigmoid(a1)\n",
    "\n",
    "        a2 = np.dot(z1, w2) + b2\n",
    "        y = common.softmax(a2)\n",
    "\n",
    "        return y\n",
    "\n",
    "        \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return common.cross_entropy_error(y, t)\n",
    "    \n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['w1'] = common.numerical_gradient(loss_w, self.params['w1'])\n",
    "        grads['b1'] = common.numerical_gradient(loss_w, self.params['b1'])\n",
    "        grads['w2'] = common.numerical_gradient(loss_w, self.params['w2'])\n",
    "        grads['b2'] = common.numerical_gradient(loss_w, self.params['b2'])\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64473dfe",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51476843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with loading images:  C:/Users/13132/Documents/Pattern_Recoginition_NN/Homework_2/ece5831-2023-assignment-2/ece5831-2023-assignment-2/Assignment_6/Train && Test_Files/train-images-idx3-ubyte.gz\n",
      "Done with loading labels:  C:/Users/13132/Documents/Pattern_Recoginition_NN/Homework_2/ece5831-2023-assignment-2/ece5831-2023-assignment-2/Assignment_6/Train && Test_Files/train-labels-idx1-ubyte.gz\n",
      "Done with loading images:  C:/Users/13132/Documents/Pattern_Recoginition_NN/Homework_2/ece5831-2023-assignment-2/ece5831-2023-assignment-2/Assignment_6/Train && Test_Files/t10k-images-idx3-ubyte.gz\n",
      "Done with loading labels:  C:/Users/13132/Documents/Pattern_Recoginition_NN/Homework_2/ece5831-2023-assignment-2/ece5831-2023-assignment-2/Assignment_6/Train && Test_Files/t10k-labels-idx1-ubyte.gz\n",
      "Current iteration is 0\n",
      "Current iteration is 1\n"
     ]
    }
   ],
   "source": [
    "mnist_data = MnistData()\n",
    "(x_train, t_train), (x_test, t_test) = mnist_data.load()\n",
    "\n",
    "iters_num = 30\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "\n",
    "train_loss = []\n",
    "\n",
    "input_size = 28*28 # 784\n",
    "Sai_Shanmukh_Varma_mnist_nn_model = two_layer_net.TwoLayerNet(input_size=input_size, hidden_size=100, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # mini-batch\n",
    "    print(\"Current iteration is {}\".format(i))\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = Sai_Shanmukh_Varma_mnist_nn_model.numerical_gradient(x_batch, t_batch)\n",
    "    for key in ('w1', 'b1', 'w2', 'b2'):\n",
    "        Sai_Shanmukh_Varma_mnist_nn_model.params[key] -= learning_rate*grad[key]\n",
    "\n",
    "loss = Sai_Shanmukh_Varma_mnist_nn_model.loss(x_batch, t_batch)\n",
    "train_loss.append(loss)\n",
    "\n",
    "\n",
    "\n",
    "with open('Sai_Shanmukh_Varma_mnist_nn_model.pkl', 'wb') as model:\n",
    "    pickle.dump(Sai_Shanmukh_Varma_mnist_nn_model,model)\n",
    "print(\"Pickle file completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09056b31",
   "metadata": {},
   "source": [
    "#### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753c9cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_image(images):\n",
    "    my_image = np.array(images)\n",
    "    my_image = my_image.reshape(784,)\n",
    "    return my_image/255\n",
    "\n",
    "def size_image(images):\n",
    "    my_image_resized = None\n",
    "    try:\n",
    "        with Image.open(images) as image:\n",
    "            my_image_grayscaled = image.convert(\"L\")\n",
    "            my_image_resized = my_image_grayscaled.resize((28,28))\n",
    "    except Exception as e:\n",
    "        print(\"Not able to process the image: {}\".format(e))\n",
    "    return my_image_resized\n",
    "\n",
    "Trained_model = None\n",
    "with open(\"Sai_Shanmukh_Varma_mnist_nn_model.pkl\", 'rb') as model:\n",
    "    Trained_model = pickle.load(model)\n",
    "\n",
    "Test_image_size = size_image(\"Test_Images/9_1.png\")\n",
    "Test_image_shape = shape_image(Test_image_size)\n",
    "\n",
    "y = Trained_model.predict(Test_image_shape)\n",
    "print(\"My trained Model {}\".format(y))\n",
    "y_hat = np.argmax(y)\n",
    "certainity = np.max(y)*100\n",
    "\n",
    "print(\"Model output is predicted as {} with a certainity of {}\".format(y_hat,certainity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49acefd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1901257e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
