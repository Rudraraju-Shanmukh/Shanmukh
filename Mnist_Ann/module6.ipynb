{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a34c8de6",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f693999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "import two_layer_net\n",
    "import common\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e08d1c7",
   "metadata": {},
   "source": [
    "#### Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64985a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "    return 1/(1 + np.exp(-a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613fd2b0",
   "metadata": {},
   "source": [
    "#### Softmax Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2771d38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    a = np.exp(a - c)\n",
    "    s = np.sum(a)\n",
    "    return a/s \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97ccfb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0] \n",
    "    return -np.sum(t * np.log(y + 1e-7))/batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "886de446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "\n",
    "    grad = np.zeros_like(x) \n",
    "\n",
    "    for idx in range(x.size):\n",
    "        # save x[idx]\n",
    "        tmp = x[idx]\n",
    "\n",
    "        # for f(x + h)\n",
    "        x[idx] = tmp + h\n",
    "        fh1 = f(x)\n",
    "\n",
    "        # for f(x - h)\n",
    "        x[idx] = tmp - h\n",
    "        fh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fh1 - fh2)/(2*h)\n",
    "        # restore x[idx]\n",
    "        x[idx] = tmp\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c3f8062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    if x.ndim == 1:\n",
    "        return _numerical_gradient(f, x)\n",
    "    else:\n",
    "        grad = np.zeros_like(x)\n",
    "        for idx, x in enumerate(x):\n",
    "            grad[idx] = _numerical_gradient(f, x)\n",
    "        return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a67906eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "        w1, w2 = self.params['w1'], self.params['w2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        a1 = np.dot(x, w1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, w2) + b2\n",
    "        y = softmax(a2)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "998f1ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(self, x, t):\n",
    "    y = self.predict(x)\n",
    "    return cross_entropy_error(y, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df49d9a7",
   "metadata": {},
   "source": [
    "#### Defining Mnist Clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90cfa085",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MnistData():   \n",
    "    image_size = 784  # 28x28\n",
    "    image_dim = (1, 28, 28)\n",
    "    train_num = 60000\n",
    "    test_num  = 10000\n",
    "    key_file = {\n",
    "        'train_images': 'C:/Users/13132/Documents/Pattern_Recoginition_NN/Homework_2/ece5831-2023-assignment-2/ece5831-2023-assignment-2/Assignment_6/Train && Test_Files/train-images-idx3-ubyte.gz',\n",
    "        'train_labels': 'C:/Users/13132/Documents/Pattern_Recoginition_NN/Homework_2/ece5831-2023-assignment-2/ece5831-2023-assignment-2/Assignment_6/Train && Test_Files/train-labels-idx1-ubyte.gz',\n",
    "        'test_images':  'C:/Users/13132/Documents/Pattern_Recoginition_NN/Homework_2/ece5831-2023-assignment-2/ece5831-2023-assignment-2/Assignment_6/Train && Test_Files/t10k-images-idx3-ubyte.gz',\n",
    "        'test_labels':  'C:/Users/13132/Documents/Pattern_Recoginition_NN/Homework_2/ece5831-2023-assignment-2/ece5831-2023-assignment-2/Assignment_6/Train && Test_Files/t10k-labels-idx1-ubyte.gz'\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def _load_images(self, file_name):\n",
    "        with gzip.open(file_name, 'rb') as f:\n",
    "            images = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        images = images.reshape(-1, self.image_size)\n",
    "\n",
    "        print('Done with loading images: ', file_name)    \n",
    "        return images\n",
    "\n",
    "    def _load_labels(self, file_name):\n",
    "        with gzip.open(file_name, 'rb') as f: \n",
    "            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        \n",
    "        print('Done with loading labels: ', file_name)    \n",
    "        return labels\n",
    "    \n",
    "    def _change_one_hot_label(self, x):\n",
    "        t = np.zeros((x.size, 10))\n",
    "        for idx, row in enumerate(t):\n",
    "            row[x[idx]] = 1\n",
    "\n",
    "        return t\n",
    "\n",
    "    def load(self, normalize=True, flatten=True, one_hot_label=True):\n",
    "        dataset = {}\n",
    "        dataset['train_images'] = self._load_images(self.key_file['train_images'])\n",
    "        dataset['train_labels'] = self._load_labels(self.key_file['train_labels'])\n",
    "        dataset['test_images']  = self._load_images(self.key_file['test_images'])\n",
    "        dataset['test_labels']  = self._load_labels(self.key_file['test_labels'])\n",
    "        \n",
    "        if normalize:\n",
    "            for key in ('train_images', 'test_images'):\n",
    "                dataset[key] = dataset[key].astype(np.float32)\n",
    "                dataset[key] /= 255.0\n",
    "    \n",
    "        if one_hot_label:\n",
    "            dataset['train_labels'] = self._change_one_hot_label(dataset['train_labels'])\n",
    "            dataset['test_labels'] = self._change_one_hot_label(dataset['test_labels'])\n",
    "    \n",
    "        if not flatten:\n",
    "             for key in ('train_images', 'test_images'):\n",
    "                dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
    "    \n",
    "        return (dataset['train_images'], dataset['train_labels']), \\\n",
    "                (dataset['test_images'], dataset['test_labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908fc68",
   "metadata": {},
   "source": [
    "#### Two Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b89963b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['w1'] = weight_init_std*np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['w2'] = weight_init_std*np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        w1, w2 = self.params['w1'], self.params['w2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1 = np.dot(x, w1) + b1\n",
    "        z1 = common.sigmoid(a1)\n",
    "\n",
    "        a2 = np.dot(z1, w2) + b2\n",
    "        y = common.softmax(a2)\n",
    "        return y\n",
    "        \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return common.cross_entropy_error(y, t)\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['w1'] = common.numerical_gradient(loss_w, self.params['w1'])\n",
    "        grads['b1'] = common.numerical_gradient(loss_w, self.params['b1'])\n",
    "        grads['w2'] = common.numerical_gradient(loss_w, self.params['w2'])\n",
    "        grads['b2'] = common.numerical_gradient(loss_w, self.params['b2'])\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb666ac1",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28995ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with loading images:  C:/Users/13132/Documents/Pattern_Recoginition_NN/Homework_2/ece5831-2023-assignment-2/ece5831-2023-assignment-2/Assignment_6/Train && Test_Files/train-images-idx3-ubyte.gz\n",
      "Done with loading labels:  C:/Users/13132/Documents/Pattern_Recoginition_NN/Homework_2/ece5831-2023-assignment-2/ece5831-2023-assignment-2/Assignment_6/Train && Test_Files/train-labels-idx1-ubyte.gz\n",
      "Done with loading images:  C:/Users/13132/Documents/Pattern_Recoginition_NN/Homework_2/ece5831-2023-assignment-2/ece5831-2023-assignment-2/Assignment_6/Train && Test_Files/t10k-images-idx3-ubyte.gz\n",
      "Done with loading labels:  C:/Users/13132/Documents/Pattern_Recoginition_NN/Homework_2/ece5831-2023-assignment-2/ece5831-2023-assignment-2/Assignment_6/Train && Test_Files/t10k-labels-idx1-ubyte.gz\n",
      "Current iteration is 0\n",
      "Current iteration is 1\n",
      "Current iteration is 2\n",
      "Current iteration is 3\n",
      "Current iteration is 4\n",
      "Current iteration is 5\n",
      "Current iteration is 6\n",
      "Current iteration is 7\n",
      "Current iteration is 8\n",
      "Current iteration is 9\n",
      "Current iteration is 10\n",
      "Current iteration is 11\n",
      "Current iteration is 12\n",
      "Current iteration is 13\n",
      "Current iteration is 14\n",
      "Current iteration is 15\n",
      "Current iteration is 16\n",
      "Current iteration is 17\n"
     ]
    }
   ],
   "source": [
    "mnist_data = MnistData()\n",
    "(x_train, t_train), (x_test, t_test) = mnist_data.load()\n",
    "\n",
    "iters_num = 30\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "train_loss = []\n",
    "input_size = 28*28\n",
    "Sai_Shanmukh_Varma_mnist_nn_model = two_layer_net.TwoLayerNet(input_size=input_size, hidden_size=100, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    print(\"Current iteration is {}\".format(i))\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    grad = Sai_Shanmukh_Varma_mnist_nn_model.numerical_gradient(x_batch, t_batch)\n",
    "    for key in ('w1', 'b1', 'w2', 'b2'):\n",
    "        Sai_Shanmukh_Varma_mnist_nn_model.params[key] -= learning_rate*grad[key]\n",
    "loss = Sai_Shanmukh_Varma_mnist_nn_model.loss(x_batch, t_batch)\n",
    "train_loss.append(loss)\n",
    "\n",
    "with open('Sai_Shanmukh_Varma_mnist_nn_model.pkl', 'wb') as model:\n",
    "    pickle.dump(Sai_Shanmukh_Varma_mnist_nn_model,model)\n",
    "print(\"Pickle file completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eccd0c9",
   "metadata": {},
   "source": [
    "#### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35f6e68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My trained Model [0.09853439 0.09911507 0.10371195 0.09808646 0.10840422 0.09695608\n",
      " 0.08917946 0.0982873  0.09931603 0.10840902]\n",
      "Model output is predicted as 9 with a certainity of 10.840902378521827\n"
     ]
    }
   ],
   "source": [
    "def shape_image(images):\n",
    "    my_image = np.array(images)\n",
    "    my_image = my_image.reshape(784,)\n",
    "    return my_image/255\n",
    "\n",
    "def size_image(images):\n",
    "    my_image_resized = None\n",
    "    try:\n",
    "        with Image.open(images) as image:\n",
    "            my_image_grayscaled = image.convert(\"L\")\n",
    "            my_image_resized = my_image_grayscaled.resize((28,28))\n",
    "    except Exception as e:\n",
    "        print(\"Not able to process the image: {}\".format(e))\n",
    "    return my_image_resized\n",
    "\n",
    "Trained_model = None\n",
    "with open(\"Sai_Shanmukh_Varma_mnist_nn_model.pkl\", 'rb') as model:\n",
    "    Trained_model = pickle.load(model)\n",
    "\n",
    "Test_image_size = size_image(\"Test_Images/9_1.png\")\n",
    "Test_image_shape = shape_image(Test_image_size)\n",
    "\n",
    "y = Trained_model.predict(Test_image_shape)\n",
    "print(\"My trained Model {}\".format(y))\n",
    "y_hat = np.argmax(y)\n",
    "certainity = np.max(y)*100\n",
    "\n",
    "print(\"Model output is predicted as {} with a certainity of {}\".format(y_hat,certainity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dfbf71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d307456b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
